---
permalink: 1560877262
title: 周志华《机器学习》第 3 章：线性模型笔记
mathjax: true
categories: 西瓜书笔记
date: 2019-06-19 01:01:02
tags:
---

> 这篇笔记

<!-- more -->

1. **线性模型的基本形式**
   给定由 $d$ 个属性描述的示例 $x = (x_1;x_2;...;x_d)$，其中 $x_i$ 是 $x$ 在第 $i$ 个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即

   $$f(x) = w_1x_1 + w_2x_2 + … + w_dx_d + b \qquad (3.1)$$    									 

   一般用向量形式写成

   $$f(x) = w^Tx + b \qquad (3.2)$$

   其中 $w=(w_1;w_2;…;w_d)$。$w$ 和 $d$ 学得之后，模型就得以确定。

2. **线性模型有很好的可解释性（comprehensibility）**

   $w$ 直观表达了各属性在预测中的重要性。例如若在西瓜问题中学得"$f_{好瓜}(x) = 0.2 · x_{色泽} + 0.5 · x_{根蒂} + 0.3 · x_{敲声} + 1$"，则意味着可通过综合考虑色泽、根蒂和敲声来判断瓜好不好，其中根蒂最要紧，而敲声比色泽更重要。

   > 解析：在西瓜问题中，模型从数据中学习到的权重分别为：0.2，0.5，0.3，不同的权重大小使得不同的特征对于预测结果有不同的影响程度。在示例中，将权重的大小排序后可得：0.5 > 0.3 > 0.2，即在判断一个瓜是否为好瓜时，特征根蒂比特征敲声重要，特征敲声比特征色泽重要。

3. **线性回归的定义**

   给定数据集 $D = \{ (x_1, y_1), (x_2, y_2), …, (x_m, y_m)  \}$，其中 $x_i = (x_{i1}; x_{i2}; … ; x_{id})$，$y_i \in \mathbb{R}$。"线性回归"（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

4. **离散属性的连续化**

   对离散属性，若属性值间存在"序"（order）关系，可通过连续化将其转化为连续值。例如二值属性"身高"的取值"高""矮"可转化为{1.0, 0.0}，三值属性"高度"的取值"高""中""低"可转化为{1.0, 0.5, 0.0}。

5. **连续属性的离散化**

   若属性值间不存在序关系，假定有 $k$ 个属性值，则通常转化为 $k$ 维向量，例如属性"瓜类"的取值"西瓜""南瓜""黄瓜"可转化为(0, 0, 1), (0, 1, 0), (1, 0, 0)。

6. **线性回归模型如何学得**

   我们先考虑一种最简单的情形：输入属性的数目只有一个。为便于讨论，此时我们忽略关于属性的下标，即 $D={(x_i, y_i)^m_{i=1}}$ ，其中 $x_i \in \mathbb{R}$。

   线性回归试图学得

   $$f(x_i) = wx_i + b，使得 f(x_i) \simeq  y_i \qquad (3.3)$$

   > 解析：线性回归模型的形式为 $f(x_i) = wx_i + b$，其中 $x_i$ 是我们用来训练模型的样本集，可以看做已知项，而 $w$ 和 $b$ 是未知项。所以我们需要知道 $w$ 和 $b$ 的值，模型才能正常使用。怎样才能求得 $w$ 和 $b$ 的值呢？

7. 线性回归模型使用的loss function

$$(w^*, b^*) = argmin_{(w, b)} \sum_{i=1}^{m}(f(x_i) - y_i)^2 $$

$$\quad\quad\quad\qquad = argmin_{(w, b)} \sum_{i=1}^{m}(y_i - wx_i - b)^2 \qquad (3.4) $$

$$\frac{\partial E_{(a, b)}}{\partial w} = 2 \left(w\sum_{i=1}^m{x_i^2} - \sum_{i=1}^m(y_i - b)x_i \right) , \qquad (3.5) $$

$$\frac{\partial E_{(a, b)}}{\partial b} = 2 \left(mb - \sum_{i=1}^m(y_i - wx_i) \right) , \qquad (3.6) $$

$$w=\frac{\sum_{i=1}^m y_i(x_i - \bar{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m} (\sum_{i=1}^m x_i)^2} \qquad (3.7) $$